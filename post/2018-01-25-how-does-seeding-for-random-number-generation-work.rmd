---
title: How does seeding for random number generation work?
metaAlignment: center
thumbnailImagePosition: left
thumbnailImage: https://i.imgur.com/IyCfoNY.gif
coverMeta: out
author: ~
date: '2018-01-25'
slug: how-does-seeding-for-random-number-generation-work
categories: [r, simulation, teaching]
tags: [randomness, seeds, animation]
---
```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Here is the problem: I'm building a simulation of a sequence of random events that leads to a specific outcome of interest only rarely. When I say rarely, I mean that the outcome I'm interested in only occurs by chance about 0.1% of the time. The simulation function I'm working with takes about 9 seconds to complete each iteration (likely indicating that I'm not the world's most efficient coder), and so if the target outcome occurs every 1 in 1000 iterations on average, then I need to wait at least `r round(9*1000/60/60,2)` hours for the simulation to run. I need to verify that the simulation is working properly, and the only way to check is to find instances of the rare outcome in the simulated data. The problem I am running into is that running the simulation a bunch of times is not statistically guaranteed to produce the rare outcome. This is a common problem in simulation: the rate of occurrence (1 in 1000) is an average rate of occurence, meaning that I likely need to simulate many more iterations for the rate of occurrence to average out to 1 in 1000. To cut to the chase, I've found that I need about 5000 iterations to get 5 occurrences, but 5000 iterations takes roughly `r round(9*5000/60/60,2)` hours to run. Who has time for that?

The way I see it, I have two choices: profile my simulation functions and speed them up, or figure out a cheat to get me the simulation results I need with fewer reps. Obviously, I'm going to spend time trying to figure out a seed hack instead of doing the real work!

I've run some tests, and it turns out that the seed that I'm setting results in a random sequence that does not produce an occurrence of the outcome of interest over the first 1000 or even 2000 iterations of the simulation. In the interest of saving time, I would ideally want to find a seed that results in a maximum number of occurrences of the target outcome over the fewest iterations possible, which is not knowable apriori. In order to halfway accomplish that goal, I realized that I needed to better understand how seeding in R works. After searching stackoverflow and crossvalidated for explanations, I was left unsatisfied. So, I decided to go way overboard and put together this entire walkthrough of random number generation in R. I probably should have profiled my functions instead, but that's the boring stuff that no one wants to read about.

## What in the world are you going on about?

If you are completely new to simulation of random sequences in R, a seed is a value that represents a starting value for a random number generator. I'll get into lots of the details of this process later in the post, but for now, you need to know that the seed value is often set arbitrarily (by picking any number) so that the simulation experiment is reproducible. Setting the seed and simulating numbers will always result in the same sequence of numbers from that specific starting point.

Without knowing anything else about this process, you way you may guess intuitively that a seed of 1 and a seed of 2 both index positions along a sequence of random numbers. By this method, two successive values of seeding (e.g., 1 and 2) might produce identical results for the second iteration of the first seed value, and the first iteration of the second seed value. We can test this posibility by running a simple experiment in R. For example, under this hypothesis, if we set the seed to 1, we would expect the second random number in the resulting sequence to be identical to the case where I set the seed to 2 and generated one number. 

If the seed values worked iteratively in this fashion, then I could just set my seed value to x + 2000 and jump 2000 iterations ahead of my crappy seed that I was using before. This would seemingly improve my chances of generating the outcome of interest over the next 3000 or so iterations, resulting in a savings of `r round(9*2000/60/60,2)` hours of run time. Let's find out if seed values have this iterative property.

### Experiment 1: Do sequential seeds index successive positions in the same random sequence?

```{r, seedexperiment1}
set.seed(1)
runif(2)
```

```{r, seedexperiment2}
set.seed(2)
runif(2)
```

Clearly, the answer to this question is no, because the second value in the first set of simulated numbers is not the same as the first value in the second set of numbers. Later, I'll get into the details of what seed values actually are and why they don't replicate numbers in the manner tested above, but for now, back to the drawing board.

### Experiment 2: Can I skip ahead in the stream of random numbers?

The next obvious question is whether or not it is possible to skip ahead in a sequence of random numbers that starts at a given seed value. Skipping ahead would allow me to bypass the early portion of the sequence that doesn't result in the outcome I'm looking for. Let's find out if we can skip ahead in the stream of random numbers that occur when I set the seed equal to 1.

```{r, seedexperiment3}
# set seed at 1
set.seed(1)
# simulate 2000 values and return the last value in the set
runif(2000)[2000]
```

Now, we need to simulate 2 new values without resetting the seed.

```{r, seedexperiment4}
# intentionally do not set the seed
runif(2)
```

Finally, here is the test: if skipping ahead in the stream works, we should be able to replicate the three values we just obtained by simulating 2002 values starting at seed 1. If skipping works, the last three values in the set below should equal the three values we obtained above (0.38, 0.87, and 0.96).

```{r, seedexperiment5}
# set the seed at 1
set.seed(1)
# simulate 2002 values and return the last 3 values in the set
runif(2002)[2000:2002]
```

**Answer:** Yes, we can skip ahead in the stream by beginning with a seed value, simulating a large number of values (n), and then continuing the simulation of the target outcome without setting a new seed. In this scenario, we are basically throwing away the first n values in the random stream and beginning from n + 1. The process is still replicable because we are still simulating values relative to a seed starting point, but we are ignoring the beginning of the random sequence. 

Here is a quick function that we can use to reliably skip ahead n iterations in a random sequence beginning from any seed value. Note that the function below works because the scope of set.seed() is global.

```{r, randomskipfunction}
seed_skip <- function(seed, n) {
        set.seed(seed)
        runif(n)
        print("skipping complete")
}

seed_skip(1, 1999)
runif(3)
```

Notice that the output of runif(3) is the same output we obtained in the random sequence skipping experiment. This means that the function worked as intended. 

Hopefully this quick demonstration has piqued your interest enough to begin thinking about some other questions related to random number generation. The remainder of this post is dedicated to providing detailed answers to some basic questions about random number generation.

## How does random number generation work?

Independent and identically distributed random numbers are required for many data science applications, especially simulation.

True random sequences are only generated by iid random variables. The sequences produced by this type of variable are non-deterministic, meaning that the next number in the sequence is always unpredictable. A random walk is a good example of this type of process.

A random walk is a set of "steps" that describes a randomly generated path through a mathematical space. In the plot shown below, 5 different random walks of 100 steps each over a 10 x 10 2D space are generated by sampling with replacement from 4 possible movements (left, right, up, down) at each step. 

```{r, loadlibraries, include=F}
library(tidyverse)
library(stringr)
library(random)
library(gganimate)
```

```{r randomwalkplot, warning = FALSE, message = FALSE}
list_of_moves <- data.frame(cbind(c(-1, 0, 1, 0), c(0, -1, 0, 1)))
colnames(list_of_moves) <- c("x", "y")
                        
random_walk <- function(steps) {
        walk <- sample_n(list_of_moves, steps, replace = TRUE)
        data.frame(cbind(x = cumsum(walk[,1]), y = cumsum(walk[,2])))
}

# simulate 5 random walks of 100 steps
set.seed(888)
walks <- NULL
steps <- 100
reps <- 5
for(i in seq_along(1:reps)){
        walk <- random_walk(steps) %>%
                mutate(row = 1:steps,
                       walk = as.character(i))
        walks <- rbind(walks, walk)
}

# create plot
p <- ggplot(walks, aes(x, y, frame = row, cumulative = TRUE, color = walk)) +
        scale_x_continuous(limits=c(-10,10)) +
        scale_y_continuous(limits=c(-10,10)) +
        theme_minimal() +
        xlab("\n x coordinates") +
        ylab("y coordinates \n") +
        ggtitle("     5 random walks over a 10 X 10 2D space") +
        theme(text=element_text(size=16)) + 
        theme(legend.position="none") + 
        geom_path(size = 2)

# animate plot
rw_animation <- gganimate(p, interval = .01, title_frame = FALSE, filename = "random_walks.gif")
```


<img align="center" src="https://i.imgur.com/IyCfoNY.gif">

Notice that the random walks appear as a tangle of overlapping movements accross the space. There is no discernable pattern to the movements, except that they all share the same origin.

## True vs. Pesudo-randomness?

Of course, a truly random walk must be generated by a true random sequence. True random sequences are produced by natural physical processes. For example, flipping a coin, rolling a die, the decay of a radioactive source, or noise in the atmosphere are all natural sources that produce true randomness.

True random sequence generation poses problems in data science applications because these sequences can be costly and difficult to generate, and reproducibility is not typically possible. There are applications of true randomness in areas such as cryptography and slot machines, but most data science applications typically leverage pseudo-random sequences. 

As discussed in *Simulation for Data Science with R*, the *random* library in R facilitates the use of true random numbers generated by atmospheric noise. The noise is collected and made available by random.org. The randomNumbers function streams the random sequence from the random.org website. The visualization below demonstrates the randomness of these truly unpredictible values.

**Note: You may only pull 10,000 random numbers at a time from random.org, and you are limited to 4 large calls to the site per day.**

```{r, randomNumbersdemo, eval = T}
x <- randomNumbers(n=10000, col = 2, min = 0, max = 1e+06, check = T)/1000000

n <- length(x)
df <- data.frame(x1 = x[1:(n-1)], x2 = x[2:n])
ggplot(df, aes(x1, x2)) +
        geom_point(size = 0.1) + 
        xlab("\n random numbers from random.org") + 
        ylab("lag 1 \n")
```

## Generating Pseudo-random Numbers and the Origin of Seed Values

Scientists have designed algorithms to generate pesudo-random sequences for over 70 years. As part of the research for the Manhattan Project in 1946, John von Neumann developed one of the first computational methods for generating pseudo-random numbers. His work was driven by the need for reproducible sequences of random numbers that did not need to be stored in memory. The algorithm he developed is called the middle squares method. 

The basic approach of the middle squares method is to choose a seed value that is n-digits in length that serves as the starting point for the algorithm. The algorithm squares the seed and then takes the middle n-digits of the result as the first number in the random sequence. Then the result becomes the new seed and the process repeats a specified number of times. Check out the 4-digit version of the middle squares method that I implemented as an R function, and the resulting pseudo-random sequences for two different seed values that I plotted below.

```{r, middlesquaresalgorithmdemo, warning = FALSE}
middle_squares <- function(sequence_length = 100, seed = 2038) {
        random_sequence <- NULL
        for(i in seq_along(1:sequence_length)) {
        # algorithm begins with any 4-digit number called the seed
                # if the seed has less than 4 digits, make it 4 digits
                seed_digits <- str_count(as.character(seed))
                seed <- as.numeric(ifelse(seed_digits < 4, paste0(rep(1, 4-seed_digits), seed), seed))
                # square the seed
                square <- seed^2
                # if the squared result has 7 digits, add a zero to the beginning to make it 8 digits
                square <- ifelse(str_count(as.character(square)) == 7, paste0(0,square), square)
                # take the middle 4 digits of the squared result as the next number in the sequence
                next_seq <- as.numeric(str_sub(square, 3, 6))
                # seed becomes the next number in the sequence
                seed <- next_seq
                # add next_seq to the master sequence
                random_sequence[i] <- next_seq
        }
        return(random_sequence)
}

# generate random sequences
x <- middle_squares()

y <- middle_squares(seed = 8437)

# build data frame for plot
df <- data.frame(cbind(x, y)) %>%
        gather(key = "starting_seed", value = "value") %>%
        mutate(starting_seed = ifelse(starting_seed == "x", "seed = 2038", "seed = 8437"),
               lag1_value = lag(value))
df$lag1_value[101] <- NA

# facet scatterplot of pseudo-random middle squares values and lagged values 
ggplot(df, aes(value, lag1_value)) + 
        geom_point(size = 2) +
        xlab("\n random values based on middle squares method") +
        ylab("lagged random values based on middle squares method \n") +
        facet_wrap(~ starting_seed, nrow = 1)
```

There are plenty of better methods of pseudo-random number generation that have been developed since the middle squares algorithm, but the idea of using a seed value as a starting point for an algorithm is an important one that is still implemented today to ensure the reproducibility of pseudo-random sequences. As a bit of trivia, the default random number generator in R is the Mersenne-Twister algorithm, which is the gold standard for pseudo-random number generation.

```{r, defaultrng}
# check default RNG
RNGkind()
```

## Linear Congruential Generators

The Mersenne-Twister is based on Linear Congruential Generators, which are relatively simple RNGs. They take 4 inputs:

1. a modulus value - m
2. a multiplier value - a
3. an increment value - c
4. a seed value - s

Here is how it works:

First, you multiply the seed by the multiplier. Then, add the increment to the result of the first operation. Finally, take the result mod the m value (the remainder when the result is divided by m). Repeating this process n times using the result of each iteration as the seed for the next run will result in a random sequence of length n with values between 0 and m-1.

```{r, linearcongruentialgenerators}
lcg <- function(n = 10, m = 4829, a = 1268, c = 8831, s = 4321){
        random_sequence <- NULL
        for(i in seq_along(1:n)) {
                # step 1: multiply the seed by the multiplier
                result <- s*a
                # step 2: add the increment value
                result <- result + c
                # step 3: take the result mod the modulus value
                result <- result %% m
                # add result to the sequence
                random_sequence[i] <- result
                # set the result as the seed for the next iteration
                s <- result
        }
        return(random_sequence)
}

# generate a sequence using the lcg
n <- 10000
x <- lcg(n = n)

# create lagged values
df <- data.frame(x1 = x[1:(n-1)], x2 = x[2:n])

# create scatterplot
ggplot(df, aes(x1, x2)) +
        geom_point(size = 2) + 
        xlab("\n random numbers from linear congruential generator") + 
        ylab("lag 1 \n")
```

**Notice anything strange about the plot? (Hint: How many points are displayed?)**

Pseudo-random number generators produce sequences that have a period, which is a fixed length after which the sequence repeats itself. Unfortunately, all pseudo-random sequences have this property; all such sequences repeat eventually. As soon as a non-unique value is produced, the deterministic nature of these algorithms (each number generated depends on the number before it) dictates that the sequence will loop from that point forward. Let's find the period of the sequence we generated in the last code chunk.

```{r, periodcalc}
period_x1 <- length(unique(df$x1))
period_x1
```

Based on the parameters chosen, after `r period_x1` values are generated, the sequence repeats itself. However, by choosing "good" values for the modulus (m) and the increment (c) it is possible to to obtain a sequence with period close to m.

### Thanks for Reading!
That's it for now, but I plan to follow up with a few other posts in this series. Next time I'll discuss Inverse Transform Sampling, and maybe the Mersenne-Twister. 

### References:

1. Templ, M. (2016). Simulation of random numbers. In [Simulation for Data Science with R](https://www.amazon.com/Simulation-Data-Science-Matthias-Templ/dp/1785881167) (pp. 89-147). Birmingham, UK: Packt Publishing. 
2. Kahn Academy. Pseudorandom number generators. <https://www.youtube.com/watch?v=GtOt7EBNEwQ>.
3. Wikipedia. John von Neumann. <https://en.wikipedia.org/wiki/John_von_Neumann>
4. Wikipedia. Middle Squares Method. <https://en.wikipedia.org/wiki/Middle-square_method>